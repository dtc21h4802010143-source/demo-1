import os
import re
import json
import logging
import numpy as np

# --- Th∆∞ vi·ªán x·ª≠ l√Ω ng√¥n ng·ªØ ---
try:
    import nltk
    from nltk.tokenize import word_tokenize
    from nltk.corpus import stopwords
except ImportError:
    import subprocess
    subprocess.check_call(['pip', 'install', 'nltk'])
    import nltk
    from nltk.tokenize import word_tokenize
    from nltk.corpus import stopwords

# --- Th∆∞ vi·ªán h·ªçc m√°y ---
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
except ImportError:
    import subprocess
    subprocess.check_call(['pip', 'install', 'scikit-learn'])
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity

# --- Th∆∞ vi·ªán to√°n h·ªçc ---
try:
    import numpy as np
except ImportError:
    import subprocess
    subprocess.check_call(['pip', 'install', 'numpy'])
    import numpy as np


class ChatbotEngine:
    def __init__(self, knowledge_base_path):
        """
        Kh·ªüi t·∫°o chatbot engine v·ªõi c∆° s·ªü tri th·ª©c JSON
        """

        # üß† ƒê·∫£m b·∫£o c√°c g√≥i d·ªØ li·ªáu NLTK ƒë∆∞·ª£c t·∫£i ƒë·∫ßy ƒë·ªß
        for resource in ['punkt', 'stopwords', 'punkt_tab']:
            try:
                nltk.download(resource, quiet=True)
            except Exception as e:
                logging.warning(f"Kh√¥ng th·ªÉ t·∫£i {resource}: {e}")

        # Th√™m ƒë∆∞·ªùng d·∫´n d·ª± ph√≤ng n·∫øu Render reset d·ªØ li·ªáu
        nltk.data.path.append(os.path.join(os.getcwd(), "nltk_data"))

        # ‚úÖ G·ªôp stopwords Anh + Vi·ªát
        self.stop_words = set(stopwords.words('english'))
        vn_stop_words = {
            'v√†', 'c·ªßa', 'cho', 'trong', 'v·ªõi', 'c√°c', 'ƒë∆∞·ª£c', 'ƒë·ªÉ', 'c√≥',
            'nh·ªØng', 'm·ªôt', 'l√†', 'n√†y', 't·ª´', 'khi', 'ƒë·∫øn', 'nh∆∞', 'kh√¥ng',
            'v·ªÅ', 't·∫°i', 'theo', 'ƒë√£', 's·∫Ω', 'v√¨', 'nh∆∞ng', 'c√≤n', 'b·ªã',
            'do', 'ph·∫£i', 'n·∫øu', 'n√™n', 'ƒë∆∞·ª£c', 'ƒëang', 'sau', 'r·ªìi', 'th√¨'
        }
        self.stop_words.update(vn_stop_words)

        # üîπ T·∫£i c∆° s·ªü tri th·ª©c
        self.knowledge_base = self.load_knowledge_base(knowledge_base_path)

        # üîπ Kh·ªüi t·∫°o vectorizer
        self.vectorizer = TfidfVectorizer(
            tokenizer=self.tokenize_text,
            stop_words=list(self.stop_words),
            lowercase=True,
            max_features=5000
        )
        self.response_vectors = None
        self.responses = []
        self.train_vectorizer()

    # ----------------------------------------------------------
    # Load d·ªØ li·ªáu
    # ----------------------------------------------------------
    def load_knowledge_base(self, path):
        try:
            with open(path, 'r', encoding='utf-8') as file:
                return json.load(file)
        except Exception as e:
            logging.error(f"L·ªói khi t·∫£i knowledge base: {e}")
            return {"intents": []}

    # ----------------------------------------------------------
    # X·ª≠ l√Ω ng√¥n ng·ªØ
    # ----------------------------------------------------------
    def tokenize_text(self, text):
        # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát, s·ªë v√† gi·ªØ l·∫°i ti·∫øng Vi·ªát
        text = re.sub(r'[^a-zA-Z√Ä-·ªπ\s]', ' ', text)
        tokens = word_tokenize(text.lower())
        tokens = [
            token for token in tokens
            if token not in self.stop_words and len(token) > 1
        ]
        return tokens

    def preprocess_text(self, text):
        return ' '.join(self.tokenize_text(text))

    # ----------------------------------------------------------
    # Hu·∫•n luy·ªán TF-IDF
    # ----------------------------------------------------------
    def train_vectorizer(self):
        corpus = []
        self.responses = []

        for intent in self.knowledge_base.get('intents', []):
            for pattern in intent.get('patterns', []):
                processed_pattern = self.preprocess_text(pattern)
                corpus.append(processed_pattern)
                self.responses.append(intent.get('responses', []))

        if corpus:
            self.response_vectors = self.vectorizer.fit_transform(corpus)
            logging.info("‚úÖ Hu·∫•n luy·ªán vectorizer th√†nh c√¥ng.")
        else:
            logging.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y pattern trong knowledge base!")

    # ----------------------------------------------------------
    # Tr·∫£ l·ªùi ng∆∞·ªùi d√πng
    # ----------------------------------------------------------
    def get_response(self, user_input, context=None):
        try:
            processed_input = self.preprocess_text(user_input)
            input_vector = self.vectorizer.transform([processed_input])
            similarities = cosine_similarity(input_vector, self.response_vectors)

            max_idx = np.argmax(similarities[0])
            max_score = similarities[0][max_idx]

            if max_score < 0.3:
                return self.get_default_response()

            response_options = self.responses[max_idx]
            return np.random.choice(response_options)

        except Exception as e:
            logging.error(f"L·ªói khi sinh ph·∫£n h·ªìi: {e}")
            return self.get_default_response()

    # ----------------------------------------------------------
    # C√¢u tr·∫£ l·ªùi m·∫∑c ƒë·ªãnh
    # ----------------------------------------------------------
    def get_default_response(self):
        return np.random.choice([
            "Xin l·ªói, t√¥i ch∆∞a hi·ªÉu c√¢u h·ªèi c·ªßa b·∫°n. B·∫°n c√≥ th·ªÉ n√≥i r√µ h∆°n kh√¥ng?",
            "T√¥i ch∆∞a r√µ √Ω b·∫°n l·∫Øm. B·∫°n c√≥ th·ªÉ gi·∫£i th√≠ch l·∫°i ƒë∆∞·ª£c kh√¥ng?",
            "B·∫°n c√≥ th·ªÉ n√≥i c·ª• th·ªÉ h∆°n ƒë·ªÉ t√¥i h·ªó tr·ª£ t·ªët h∆°n kh√¥ng?"
        ])

    # ----------------------------------------------------------
    # C·∫≠p nh·∫≠t tri th·ª©c
    # ----------------------------------------------------------
    def update_knowledge_base(self, new_intent):
        try:
            self.knowledge_base.setdefault('intents', []).append(new_intent)
            self.train_vectorizer()
            logging.info("‚úÖ ƒê√£ c·∫≠p nh·∫≠t knowledge base.")
            return True
        except Exception as e:
            logging.error(f"L·ªói khi c·∫≠p nh·∫≠t knowledge base: {e}")
            return False

    # ----------------------------------------------------------
    # C√°c ch·ª©c nƒÉng m·ªü r·ªông
    # ----------------------------------------------------------
    def save_interaction(self, user_input, bot_response, feedback=None):
        # TODO: Ghi l·ªãch s·ª≠ h·ªôi tho·∫°i v√†o DB ho·∫∑c file log
        pass

    def get_program_recommendations(self, user_preferences):
        # TODO: G·ª£i √Ω ch∆∞∆°ng tr√¨nh h·ªçc d·ª±a theo s·ªü th√≠ch
        pass
